import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import os
import itertools

class GradientReversalFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0):
        return GradientReversalFunction.apply(x, alpha)

class FeatureExtractor(nn.Module):
    def __init__(self):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(7, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )

    def forward(self, x):
        return self.network(x)

class LabelPredictor(nn.Module):
    def __init__(self):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.network(x)

class DomainClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

    def forward(self, x):
        return self.network(x)

def kge(y_true, y_pred):
    r = np.corrcoef(y_true, y_pred)[0, 1]
    beta = np.mean(y_pred) / np.mean(y_true)
    gamma = (np.std(y_pred)/np.mean(y_pred)) / (np.std(y_true)/np.mean(y_true))
    return 1 - np.sqrt((r-1)**2 + (beta-1)**2 + (gamma-1)**2)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

data = pd.read_csv('D:/train_data.csv') 
features = ['TA', 'TArange', 'VPD', 'WS', 'SSM', 'RSDN', 'LAI']

target = 'ET'
site_ids = data['SITE_ID'].unique()

results_path = 'D:/results.csv'

def adjust_lambda(progress):
    if progress < 0.2:
        return 0.0
    else:
        adjusted_progress = (progress - 0.2) / 0.8
        return 2 / (1 + np.exp(-10 * adjusted_progress)) - 1

def train_model_with_warmup_and_weight(test_site_id):
    train_data = data[data['SITE_ID'] != test_site_id]
    test_data = data[data['SITE_ID'] == test_site_id]

    X_train = train_data[features].values
    y_train = train_data[target].values.reshape(-1, 1)
    X_test = test_data[features].values
    y_test = test_data[target].values.reshape(-1, 1)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    n_source = X_train.shape[0]
    n_target = X_test.shape[0]
    source_weight = 1.0 / (2 * n_source)  
    target_weight = 1.0 / (2 * n_target)

    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)
    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)

    source_loader = DataLoader(TensorDataset(X_train_t, y_train_t), 
                             batch_size=64, shuffle=True)
    target_loader = itertools.cycle(DataLoader(TensorDataset(X_test_t), 
                                   batch_size=64, shuffle=True))

    feature_extractor = FeatureExtractor().to(device)
    label_predictor = LabelPredictor().to(device)
    domain_classifier = DomainClassifier().to(device)
    grl = GradientReversalLayer()

    optimizer = optim.Adam(list(feature_extractor.parameters()) +
                         list(label_predictor.parameters()) +
                         list(domain_classifier.parameters()), lr=0.001)
    criterion_reg = nn.MSELoss()
    criterion_domain = nn.CrossEntropyLoss(reduction='none') 

    num_epochs = 50
    for epoch in range(num_epochs):
        feature_extractor.train()
        total_loss = 0.0
        progress = epoch / num_epochs
        lambda_val = adjust_lambda(progress) 

        for src_X, src_y in source_loader:
            tgt_X = next(target_loader)[0].to(device)

            src_feat = feature_extractor(src_X)
            tgt_feat = feature_extractor(tgt_X)

            pred = label_predictor(src_feat)
            loss_reg = criterion_reg(pred, src_y)

            combined_feat = torch.cat([src_feat, tgt_feat])
            domain_input = grl(combined_feat, alpha=lambda_val)
            domain_pred = domain_classifier(domain_input)

            batch_source = src_X.size(0)
            batch_target = tgt_X.size(0)
            weights = torch.cat([
                torch.full((batch_source,), source_weight, device=device),
                torch.full((batch_target,), target_weight, device=device)
            ])

            domain_labels = torch.cat([
                torch.zeros(batch_source, dtype=torch.long),
                torch.ones(batch_target, dtype=torch.long)
            ]).to(device)
            
            loss_domain = (criterion_domain(domain_pred, domain_labels) * weights).mean()
            loss = loss_reg + lambda_val * loss_domain

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()

        print(f'Epoch [{epoch+1:02d}/{num_epochs}] Loss: {total_loss/len(source_loader):.4f} '
              f'Lambda: {lambda_val:.2f}')

    feature_extractor.eval()
    with torch.no_grad():
        test_feat = feature_extractor(X_test_t)
        y_pred = label_predictor(test_feat).squeeze().cpu().numpy()
        y_true = y_test_t.squeeze().cpu().numpy()
        return kge(y_true, y_pred)

results = []
for site_index, test_site_id in enumerate(site_ids):
    print(f'\nProcessing site {site_index+1}/{len(site_ids)}: {test_site_id}')
    kge_score = train_model_with_warmup_and_weight(test_site_id)
    results.append({'SITE_ID': test_site_id, 'KGE': kge_score})

pd.DataFrame(results).to_csv(results_path, index=False)
print(f'Results saved to {results_path}')

# ==================== DANN-2 ============================================================================================================= #
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from informer import Informer 
from tqdm import tqdm
import itertools
import warnings


warnings.filterwarnings("ignore", category=DeprecationWarning)

class GradientReversalFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.alpha, None

class GradientReversalLayer(nn.Module):
    def forward(self, x, alpha=1.0):
        return GradientReversalFunction.apply(x, alpha)

class TimeSeriesProcessor:
    def __init__(self, window_size=15):
        self.window_size = window_size
        self.features = ['TA', 'TArange', 'VPD', 'WS', 'SSM', 'RSDN', 'LAI']
        self.target = 'ET'

    def _create_sequence(self, data):
        sequences = []
        for i in range(len(data)-self.window_size):
            seq = data[self.features].iloc[i:i+self.window_size].values
            target = data[self.target].iloc[i+self.window_size]
            sequences.append({
                'features': seq.astype(np.float32),
                'target': target.astype(np.float32)
            })
        return sequences

    def process_data(self, df):
        processed = []
        for site_id, group in df.groupby('SITE_ID'):
            group = group.sort_index()
            seq_df = pd.DataFrame(self._create_sequence(group))
            seq_df['SITE_ID'] = site_id
            processed.append(seq_df)
        
        full_data = pd.concat(processed, ignore_index=True)
        
        scaler = StandardScaler()
        flat_features = np.vstack(full_data['features'].values)
        scaled = scaler.fit_transform(flat_features)
        full_data['features'] = list(scaled.reshape((-1, self.window_size, len(self.features))))
        
        return full_data


class HydroDataset(Dataset):
    def __init__(self, data):
        self.features = np.stack(data['features'].values)
        self.targets = data['target'].values
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return (
            torch.tensor(self.features[idx], dtype=torch.float32),
            torch.tensor(self.targets[idx], dtype=torch.float32)
        )

class DAformer(nn.Module):
    def __init__(self, input_dim=7, window_size=15, d_model=64):
        super().__init__()

        self.encoder = Informer(
            input_size=input_dim,
            output_size=1,
            seq_len=window_size,
            label_len=0,
            factor=5,
            d_model=d_model,
            n_heads=4,
            e_layers=2,
            d_layers=1,
            activation='gelu'
        ).encoder
        

        self.grl = GradientReversalLayer()
        self.domain_cls = nn.Sequential(
            nn.Linear(d_model, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )
        

        self.regressor = nn.Sequential(
            nn.Linear(d_model, 32),
            nn.ReLU(),
            nn.Linear(32, 1))

    def forward(self, x, alpha=1.0):

        x = x.permute(1, 0, 2)
        enc_out, _ = self.encoder(x) 
        last_hidden = enc_out[-1]
        

        domain_feat = self.grl(last_hidden, alpha)
        domain_logits = self.domain_cls(domain_feat)

        pred = self.regressor(last_hidden)
        return pred.squeeze(), domain_logits

def adjust_lambda(progress):

    if progress < 0.2:
        return 0.0
    adjusted_progress = (progress - 0.2) / 0.8
    return 2 / (1 + np.exp(-10 * adjusted_progress)) - 1

def train_daformer(test_site, full_data, window_size=15):

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    

    source_data = full_data[full_data['SITE_ID'] != test_site]
    target_data = full_data[full_data['SITE_ID'] == test_site]
    
    source_loader = DataLoader(
        HydroDataset(source_data),
        batch_size=32,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    target_loader = itertools.cycle(DataLoader(
        HydroDataset(target_data),
        batch_size=32,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    ))
    

    model = DAformer(
        input_dim=source_data['features'].iloc[0].shape[1],
        window_size=window_size
    ).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    mse_loss = nn.MSELoss()
    ce_loss = nn.CrossEntropyLoss()


    num_epochs = 50
    best_kge = -np.inf
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        progress = epoch / num_epochs
        lambda_val = adjust_lambda(progress)
        
        pbar = tqdm(source_loader, desc=f'Epoch {epoch+1:03d}', leave=False)
        for src_x, src_y in pbar:
            tgt_x, _ = next(target_loader)
            

            src_x, src_y = src_x.to(device), src_y.to(device)
            tgt_x = tgt_x.to(device)
            

            src_pred, src_domain = model(src_x, lambda_val)
            tgt_pred, tgt_domain = model(tgt_x, lambda_val)
            

            reg_loss = mse_loss(src_pred, src_y)
            

            domain_labels = torch.cat([
                torch.zeros(src_x.size(0)), 
                torch.ones(tgt_x.size(0))
            ]).long().to(device)
            domain_logits = torch.cat([src_domain, tgt_domain])
            domain_loss = ce_loss(domain_logits, domain_labels)
            

            loss = reg_loss + lambda_val * domain_loss
            

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({'Loss': f"{loss.item():.4f}"})
        
        model.eval()
        with torch.no_grad():
            X_test = torch.tensor(np.stack(target_data['features']), 
                                dtype=torch.float32).to(device)
            y_test = torch.tensor(target_data['target'].values, 
                                dtype=torch.float32).to(device)
            preds, _ = model(X_test)
            current_kge = kge(y_test.cpu().numpy(), preds.cpu().numpy())
            
        print(f"Epoch {epoch+1:03d} | Loss: {total_loss/len(source_loader):.4f} | "
              f"Lambda: {lambda_val:.3f} | KGE: {current_kge:.4f}")
        
        if current_kge > best_kge:
            best_kge = current_kge
            torch.save(model.state_dict(), f"best_model_{test_site}.pth")

    return best_kge


def kge(y_true, y_pred):
    r = np.corrcoef(y_true, y_pred)[0, 1]
    beta = np.mean(y_pred) / np.mean(y_true)
    gamma = (np.std(y_pred)/np.mean(y_pred)) / (np.std(y_true)/np.mean(y_true))
    return 1 - np.sqrt((r-1)**2 + (beta-1)**2 + (gamma-1)**2)

if __name__ == "__main__":

    DATA_PATH = "D:/train_data.csv"
    WINDOW_SIZE = 15
    

    print("Loading raw data...")
    raw_data = pd.read_csv(DATA_PATH, index_col=0)  
    
    print("Processing time series...")
    processor = TimeSeriesProcessor(window_size=WINDOW_SIZE)
    processed_data = processor.process_data(raw_data)
    

    print("Starting cross validation...")
    results = []
    sites = processed_data['SITE_ID'].unique()
    
    for site in tqdm(sites, desc="Sites Progress"):
        try:
            kge_score = train_daformer(site, processed_data, WINDOW_SIZE)
            results.append({'Site': site, 'KGE': kge_score})
        except Exception as e:
            print(f"Error processing {site}: {str(e)}")
            results.append({'Site': site, 'KGE': np.nan})
    

    pd.DataFrame(results).to_csv("results2.csv", index=False)
    print("All done!")
